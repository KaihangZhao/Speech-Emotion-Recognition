{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport librosa # audio processing\nfrom IPython.display import Audio # playing audio\nfrom matplotlib import pyplot as plt # plots\nimport librosa.display\n\n!pip install noisereduce\nimport noisereduce as nr\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport seaborn as sns\n\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input/speech-emotion-recognition-en/Crema'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-19T19:30:10.803702Z","iopub.execute_input":"2022-06-19T19:30:10.804390Z","iopub.status.idle":"2022-06-19T19:30:20.505736Z","shell.execute_reply.started":"2022-06-19T19:30:10.804347Z","shell.execute_reply":"2022-06-19T19:30:20.504922Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"code","source":"data = [] # the audio signal\nlabel = [] # the sentiment (for classification)\nmeta = [] # metadata (actor_sentence_sentiment_pitch)\nsampling_rate = 18000 # all of them should have the same sampling rate","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:36.916536Z","iopub.execute_input":"2022-06-19T18:32:36.918492Z","iopub.status.idle":"2022-06-19T18:32:36.927221Z","shell.execute_reply.started":"2022-06-19T18:32:36.918449Z","shell.execute_reply":"2022-06-19T18:32:36.926485Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def play_plot(index):\n    print(meta[index])\n    Audio(data=data[index], rate=sampling_rate)\n    #fig, ax = plt.subplots(nrows=3, sharex=True)\n    #librosa.display.waveshow(data[index], sr=sampling_rate, ax=ax[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:43.557549Z","iopub.execute_input":"2022-06-19T18:32:43.557799Z","iopub.status.idle":"2022-06-19T18:32:43.562519Z","shell.execute_reply.started":"2022-06-19T18:32:43.557770Z","shell.execute_reply":"2022-06-19T18:32:43.561544Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def adjust_length(time_series_list, length):\n    n = len(time_series_list)\n    for i in range(n):\n        audio_length = len(time_series_list[i])\n        if audio_length < length:\n            time_series_list[i] = np.append(time_series_list[i], [0 for i in range(length-audio_length)])\n        else:\n            time_series_list[i] = np.array(time_series_list[i][:length])","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:45.400945Z","iopub.execute_input":"2022-06-19T18:32:45.401683Z","iopub.status.idle":"2022-06-19T18:32:45.407679Z","shell.execute_reply.started":"2022-06-19T18:32:45.401644Z","shell.execute_reply":"2022-06-19T18:32:45.406835Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def check_for_nan(l):\n    for x in l:\n        if str(x) == 'nan':\n            return True\n    return False","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:48.021963Z","iopub.execute_input":"2022-06-19T18:32:48.022221Z","iopub.status.idle":"2022-06-19T18:32:48.026761Z","shell.execute_reply.started":"2022-06-19T18:32:48.022194Z","shell.execute_reply":"2022-06-19T18:32:48.025700Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"emotions_dict = dict()\nemotions_dict['SAD'] = 0\nemotions_dict['ANG'] = 1\nemotions_dict['DIS'] = 2\nemotions_dict['FEA'] = 3\nemotions_dict['HAP'] = 4\nemotions_dict['NEU'] = 5","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:53.932769Z","iopub.execute_input":"2022-06-19T18:32:53.933358Z","iopub.status.idle":"2022-06-19T18:32:53.938054Z","shell.execute_reply.started":"2022-06-19T18:32:53.933314Z","shell.execute_reply":"2022-06-19T18:32:53.937115Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"'''Load to lists.. takes too long, run it just once''' \nlength_sum = 0\nlist_a = []\nlist_b = []\nfor dirname, _, filenames in os.walk('/kaggle/input/speech-emotion-recognition-en/Crema'):\n    for filename in filenames:\n        meta.append(filename[:-4])\n        full_filename = os.path.join(dirname, filename)\n        sentiment = filename.split('_')[2]\n        label.append(emotions_dict[sentiment])\n        signal, sr = librosa.load(full_filename, sr = sampling_rate)\n        reduced_noise = nr.reduce_noise(y=signal, sr=sampling_rate)\n        if not check_for_nan(reduced_noise):\n            signal = reduced_noise\n        data.append(signal)\n        length_sum += len(signal)\n        if (len(data)%100 == 0):\n            print(len(data), \" audio loaded\")\nn = len(data)\nadjust_length(data, 3*sampling_rate)\ndata = np.array(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:32:56.975073Z","iopub.execute_input":"2022-06-19T18:32:56.975733Z","iopub.status.idle":"2022-06-19T18:48:57.887107Z","shell.execute_reply.started":"2022-06-19T18:32:56.975694Z","shell.execute_reply":"2022-06-19T18:48:57.886275Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Display metadata, play audio and plot waveform","metadata":{}},{"cell_type":"code","source":"index = 5","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:48:57.888887Z","iopub.execute_input":"2022-06-19T18:48:57.889187Z","iopub.status.idle":"2022-06-19T18:48:57.893418Z","shell.execute_reply.started":"2022-06-19T18:48:57.889150Z","shell.execute_reply":"2022-06-19T18:48:57.892660Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"Audio(data=data[index], rate=sampling_rate)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:48:57.895127Z","iopub.execute_input":"2022-06-19T18:48:57.895717Z","iopub.status.idle":"2022-06-19T18:48:57.917016Z","shell.execute_reply.started":"2022-06-19T18:48:57.895679Z","shell.execute_reply":"2022-06-19T18:48:57.916320Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nfig.suptitle(meta[index], fontsize=15)\nlibrosa.display.waveshow(data[index], sr=sampling_rate)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:48:57.918560Z","iopub.execute_input":"2022-06-19T18:48:57.918809Z","iopub.status.idle":"2022-06-19T18:48:58.320003Z","shell.execute_reply.started":"2022-06-19T18:48:57.918773Z","shell.execute_reply":"2022-06-19T18:48:58.319231Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def feature_extraction_1D(data):\n\n    # Zero Crossing rate\n    features = librosa.feature.zero_crossing_rate(y=data)\n\n    # Energy\n    features = np.append(features, librosa.feature.rms(y=data), axis=1)\n\n    # Mel-frequency cepstral coefficient\n    l = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0).reshape(1, 106)\n    features = np.append(features, l, axis=1)\n    \n    # Spectral Centroid\n    features = np.append(features, librosa.feature.spectral_centroid(y=data, sr=sampling_rate), axis=1)\n    \n    # Spectral Bandwidth\n    features = np.append(features, librosa.feature.spectral_bandwidth(y=data, sr=sampling_rate), axis=1)\n    \n    # Spectral Flatness\n    features = np.append(features, librosa.feature.spectral_flatness(y=data), axis=1)\n    \n    # Spectral Rolloff maximum frequencies\n    features = np.append(features, librosa.feature.spectral_rolloff(y=data, sr=sampling_rate), axis=1)\n    \n    # Spectral Rolloff minimum frequencies\n    features = np.append(features, librosa.feature.spectral_rolloff(y=data, sr=sampling_rate, roll_percent=0.01), axis=1)\n    \n    return np.array(features)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:48:58.321387Z","iopub.execute_input":"2022-06-19T18:48:58.321787Z","iopub.status.idle":"2022-06-19T18:48:58.331281Z","shell.execute_reply.started":"2022-06-19T18:48:58.321749Z","shell.execute_reply":"2022-06-19T18:48:58.330339Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_features_extracted_1D = []\nfor i in range(n):\n    data_features_extracted_1D.append(np.squeeze(np.append(feature_extraction_1D(data[i]), label[i])))\n    if (len(data_features_extracted_1D)%100 == 0):\n            print(len(data_features_extracted_1D), \" entry processed\")\ndata_features_extracted_1D = np.array(data_features_extracted_1D)\nprint(data_features_extracted_1D.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T18:48:58.332554Z","iopub.execute_input":"2022-06-19T18:48:58.333354Z","iopub.status.idle":"2022-06-19T18:54:47.945367Z","shell.execute_reply.started":"2022-06-19T18:48:58.333318Z","shell.execute_reply":"2022-06-19T18:54:47.944581Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Split Data","metadata":{}},{"cell_type":"code","source":"def split_1D(x,y):\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state =1, stratify = y)\n    return x_train, x_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:11:00.986279Z","iopub.execute_input":"2022-06-19T19:11:00.986549Z","iopub.status.idle":"2022-06-19T19:11:00.991387Z","shell.execute_reply.started":"2022-06-19T19:11:00.986518Z","shell.execute_reply":"2022-06-19T19:11:00.990654Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = split_1D(data_features_extracted_1D, label)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:11:01.614180Z","iopub.execute_input":"2022-06-19T19:11:01.614433Z","iopub.status.idle":"2022-06-19T19:11:01.655365Z","shell.execute_reply.started":"2022-06-19T19:11:01.614404Z","shell.execute_reply":"2022-06-19T19:11:01.654607Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:11:02.032985Z","iopub.execute_input":"2022-06-19T19:11:02.033250Z","iopub.status.idle":"2022-06-19T19:11:02.038925Z","shell.execute_reply.started":"2022-06-19T19:11:02.033220Z","shell.execute_reply":"2022-06-19T19:11:02.037992Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"x.squeeze","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = torch.FloatTensor(x)\n        self.y = torch.tensor(y)\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, idx):\n        return self.x[idx,:], self.y[idx]\n    \ntrain_ds = AudioDataset(x_train, y_train)\ntest_ds = AudioDataset(x_test, y_test)\ntrain_dl = DataLoader(train_ds, batch_size = 100,shuffle = True)\ntest_dl = DataLoader(test_ds, batch_size = 100,shuffle = False)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:28:10.813751Z","iopub.execute_input":"2022-06-19T19:28:10.814319Z","iopub.status.idle":"2022-06-19T19:28:10.839549Z","shell.execute_reply.started":"2022-06-19T19:28:10.814280Z","shell.execute_reply":"2022-06-19T19:28:10.838861Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 =self.audio() \n        self.layer2=self.linear()\n        \n    def audio(self):\n        layer=nn.Sequential(  \n            nn.Conv1d(in_channels=1,out_channels=64,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=3),\n            nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=2),\n            nn.Conv1d(in_channels=64,out_channels=128,kernel_size=3,padding=1),nn.ReLU(),\n            nn.Conv1d(in_channels=128,out_channels=128,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=2),\n            nn.Conv1d(in_channels=128,out_channels=256,kernel_size=3,padding=1),nn.ReLU(),\n            nn.Conv1d(in_channels=256,out_channels=256,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=2),\n            nn.Conv1d(in_channels=256,out_channels=512,kernel_size=3,padding=1),nn.ReLU(),\n            nn.Conv1d(in_channels=512,out_channels=512,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=2),\n            nn.Conv1d(in_channels=512,out_channels=512,kernel_size=3,padding=1),nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2,stride=2)\n        )\n        return layer\n    def linear(self):\n        layer=nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(4096,2048),nn.ReLU(),\n                nn.Dropout(0.5),\n                nn.Linear(2048,6)\n                )\n        return layer\n        \n    def forward(self,x):\n        x=self.layer1(x)\n        x=self.layer2(x)\n        return x\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:27:37.969210Z","iopub.execute_input":"2022-06-19T19:27:37.969732Z","iopub.status.idle":"2022-06-19T19:27:37.985325Z","shell.execute_reply.started":"2022-06-19T19:27:37.969694Z","shell.execute_reply":"2022-06-19T19:27:37.984562Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, train_dl):\n    device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n    train_loss = 0\n    for X, y in train_dl:\n        model.train()\n        X = X.unsqueeze(1).to(device)\n        y = y.to(device)\n        y_pred = model(X)\n        loss = F.cross_entropy(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item() * X.size(0)\n        torch.cuda.empty_cache()\n    train_loss = train_loss / len(train_dl.dataset)\n    return train_loss\n\n\ndef test(model, test_dl):\n    device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n    test_loss = 0\n    accuracy = 0\n    for X, y in test_dl:\n        X = X.unsqueeze(1).to(device)\n        y = y.to(device)\n        model.eval()\n        y_pred = model(X)\n        loss = F.cross_entropy(y_pred, y)\n\n        test_loss += loss.item() * X.size(0)\n        accuracy += sum(y_pred.argmax(dim=1) == y)\n        torch.cuda.empty_cache()\n    # calculate accuracy and loss\n    test_loss = test_loss / len(test_dl.dataset)\n    accuracy = accuracy / len(test_dl.dataset)\n    return test_loss, accuracy.item()\n\n\ndef train_loop(model, optimizer, train_dl, test_dl, epoch):\n    for i in range(epoch):\n        train_loss = train_one_epoch(model, optimizer, train_dl)\n        test_loss, test_acc = test(model, test_dl)\n        print(\n            f\"\"\"train loss:{round(train_loss, 3)}, test loss: {round(test_loss, 3)}, test acc: {round(test_acc, 3)}\"\"\")","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:30:20.509280Z","iopub.execute_input":"2022-06-19T19:30:20.509510Z","iopub.status.idle":"2022-06-19T19:30:20.521564Z","shell.execute_reply.started":"2022-06-19T19:30:20.509476Z","shell.execute_reply":"2022-06-19T19:30:20.520714Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"model = CNN().cuda()\noptimizer=torch.optim.Adam(model.parameters(),lr=0.00005,weight_decay=5e-3) \ntrain_loop(model,optimizer,train_dl,test_dl,30)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:35:23.349092Z","iopub.execute_input":"2022-06-19T19:35:23.349522Z","iopub.status.idle":"2022-06-19T19:36:25.233204Z","shell.execute_reply.started":"2022-06-19T19:35:23.349484Z","shell.execute_reply":"2022-06-19T19:36:25.232438Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"## RESNET","metadata":{}},{"cell_type":"code","source":"class ResNetblock(nn.Module):\n    def __init__(self,input_channel,out_channel,stride=1, convx=False):\n        super(ResNetblock,self).__init__()\n        self.C1 = nn.Conv1d(input_channel,out_channel,kernel_size=3,padding=1,stride=stride)\n        self.B1 = nn.BatchNorm1d(out_channel)\n        self.relu = nn.ReLU()\n        self.C2 = nn.Conv1d(out_channel,out_channel,kernel_size=3,padding=1)\n        self.B2 = nn.BatchNorm1d(out_channel)\n\n        if convx:\n            self.CX = nn.Conv1d(input_channel,out_channel,kernel_size=1,stride=stride)\n        else:\n            self.CX=None\n\n    def forward(self,X):\n        Y = self.relu(self.B1(self.C1(X)))\n        Y = self.B2(self.C2(Y))\n        if self.CX:\n            X = self.CX(X)\n        Y+=X\n        return F.relu(Y)\n\nclass ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet,self).__init__()\n        self.b1 = self.head_(1)\n        self.b2 = nn.Sequential(*self.resnetblock_(64, 64, 2, first=True))\n        self.b3 = nn.Sequential(*self.resnetblock_(64, 128, 2))\n        self.b4 = nn.Sequential(*self.resnetblock_(128, 256, 2))\n        self.b5 = nn.Sequential(*self.resnetblock_(256, 256, 2))\n        self.b6 = nn.Sequential(*self.resnetblock_(256, 512, 2))\n        self.connect = nn.Sequential(nn.Flatten(),\n                                     nn.Linear(4608,512),nn.ReLU(),\n                                     nn.Dropout(0.5),\n                                     nn.Linear(512,6)\n                                    )\n                        \n    def forward(self,X):\n        X=self.b1(X)\n        X=self.b2(X)\n        X=self.b3(X)\n        X=self.b4(X)\n        X=self.b5(X)\n        X=self.b6(X)\n        X=self.connect(X)\n        return X\n        \n        \n        \n        \n    def head_(self,input_channel):\n        head = nn.Sequential(nn.Conv1d(input_channel,64,kernel_size=7,stride=3,padding=3),\n                               nn.BatchNorm1d(64),\n                               nn.MaxPool1d(kernel_size=3,padding=1,stride=2))\n        return head\n\n    def resnetblock_(self,input_channel,output_channel,num_res,first = False):\n        block=[]\n        for i in range(num_res):\n            if i==0 and not first:\n                block.append(ResNetblock(input_channel,output_channel,stride=2,convx=True))\n            else:\n                block.append(ResNetblock(output_channel,output_channel))\n        return block\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:53:34.103556Z","iopub.execute_input":"2022-06-19T19:53:34.103805Z","iopub.status.idle":"2022-06-19T19:53:34.120257Z","shell.execute_reply.started":"2022-06-19T19:53:34.103775Z","shell.execute_reply":"2022-06-19T19:53:34.119580Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"model = ResNet().cuda()\noptimizer=torch.optim.Adam(model.parameters(),lr=0.00005,weight_decay=5e-3) \ntrain_loop(model,optimizer,train_dl,test_dl,10)","metadata":{"execution":{"iopub.status.busy":"2022-06-19T19:57:19.924804Z","iopub.execute_input":"2022-06-19T19:57:19.925392Z","iopub.status.idle":"2022-06-19T19:57:48.507215Z","shell.execute_reply.started":"2022-06-19T19:57:19.925353Z","shell.execute_reply":"2022-06-19T19:57:48.505796Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}